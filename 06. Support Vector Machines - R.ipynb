{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "Exercise 6 - Support Vector Machines\n===\n\nSupport vector machines (SVMs) let us predict categories. This exercise will demonstrate a simple support vector machine that can predict a category from a small number of features. \n\nOur problem is that we want to be able to categorise which type of tree a new specimen belongs to. To do this, we will use leaf and trunk features of three different types of trees to train SVMs.\n\n#### Run the cell below to load the required libraries"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Run this!\n\n# Load required packages\nsuppressMessages(install.packages(\"tidyverse\"))\nsuppressMessages(library(\"tidyverse\"))\n\nsuppressMessages(install.packages(\"e1071\"))\nsuppressMessages(library(\"e1071\"))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Step 1\n---\n\nFirst, let's load the required packages for this session, and load the raw data to see what features we have.\n\n**In the code below, replace `<dataStructure>` with `str` to view the structure of the raw data. Run the code once complete.**"
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Load tree data and save as a new variable named `tree_data`\ntree_data <- read.csv(\"Data/trees.csv\")\n\n###\n# IN THE CODE BELOW, CHECK THE STRUCTURE OF tree_data BY REPLACING <dataStructure> WITH str\n###\n<dataStructure>(tree_data)\n###",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Given the results from `str(tree_data)`, we can see that we have _four features_: \n\n* `leaf_width`\n* `leaf_length`\n* `trunk_girth`\n* `trunk_height`\n\nWe also have _one label_:\n\n* `tree_type`\n\nLet's plot these features using the package `ggplot2`. We will look at the leaf features and trunk features separately using scatter plots, and colour the points based on the label `tree_type`.\n\n### In the cell below replace:\n#### 1. `<xData>` with `leaf_width`\n#### 2. `<yData>` with `leaf_length`\n#### then __run the code__."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Plot the leaf features, where `x = leaf_width` and `y = leaf_length`\ntree_data %>%\n\n###\n# REPLACE <xData> WITH leaf_width and <yData> with leaf_length\n###\nggplot(aes(x = <xData>, y = <yData>, colour = as.factor(tree_type))) +\ngeom_point() +\nggtitle(\"Leaf length vs. leaf width coloured by tree type\") +\nlabs(x = \"Leaf width\", y = \"Leaf length\", colour = \"Tree type\") +\ntheme(plot.title = element_text(hjust = 0.5))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Based on the features `leaf_width` and `leaf_length`, we can see three groups that separate according to the label `tree_type`: `0`, `1`, and `2` (coloured red, green, and blue, respectively).\n\nNow let's plot the trunk features in a separate plot.\n\nIn the code below, we will graph each of the trunk features.\n\n### In the cell below replace:\n#### 1. `<xData>` with `trunk_girth`\n#### 2. `<yData>` with `trunk_height`\n#### then __run the code__."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Plot the trunk features, where `x = trunk_girth` and `y = trunk_height`\ntree_data %>%\n\n###\n# REPLACE <xData> WITH trunk_girth and <yData> WITH trunk_height\n###\nggplot(aes(x = <xData>, y = <yData>, colour = as.factor(tree_type))) +\n###\ngeom_point() +\nggtitle(\"Trunk height vs. trunk girth coloured by tree type\") +\nlabs(x = \"Trunk girth\", y = \"Trunk height\", colour = \"Tree type\") +\ntheme(plot.title = element_text(hjust = 0.5))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Based on the features `trunk_girth` and `trunk_height`, again we can see three groups that separate according to the label `tree_type`: `0`, `1`, and `2` (coloured red, green, and blue, respectively). There are some outliers, but for the most part, the features trunk girth and trunk height allow you to predict tree type.\n\nNow, say we obtain a new tree specimen and we want to figure out the tree type based on its leaf and trunk measurements. We *could* make a rough guess as to which tree type it belongs to based on where the tree data points lie in the two scatter plots we just created. Alternatively, using these same leaf and trunk measurements, SVMs can predict the tree type for us. SVMs will use the features and labels we provide for known tree types to create hyperplanes for tree type. These hyperplanes allow us to predict which tree type a new tree specimen belongs to, given their leaf and trunk measurements.\n\nIn the next step, we will use SVMs to help solve this problem.\n\nStep 2\n-----\n\nLet's make two SVMs using our data, `tree_data`: one SVM based on the leaf features, and another SVM based on the trunk features.\n\nThe syntax for a simple SVM using the package `e1071` is as follows:\n\n`svm_model <- svm(x = x, y = y, data = dataset)`\n\nwhere `x` represents the features (of class *matrix*), and `y` represents the labels (of class *factor*).\n\n> **R uses a variety of data types and data structures to describe different objects. You may have noticed a few types of objects already, including** `data.frame`, `list`, `matrix`, `factor`, **and** atomic vectors (`integer`, `numeric`, `logical`, `character`). **Knowing the structure of your data object is crucial, particularly when you are running functions that require the data object to be of a certain type.**\n\n> **For the `svm` function, we require two types of data structures: a** `matrix` **and a** `factor`**. A** `matrix` **is a two-dimensional data structure (with rows and columns) containing elements of all the same type, most commonly numbers (numeric or integer) with which you can perform further mathematical operations. Note that a** `matrix` **is different from a** `data.frame`**, as data frames can contain a mix of elements, i.e. both numbers (numeric/integer) and letters (factor/character/logical). A** `factor` **is used to categorize data, where the names of the categories are known as levels. For example, \"fruit\" could be a factor, with levels including \"apples\", \"bananas\", and \"oranges\", or in our example \"tree type\" can be a factor, with levels \"0\", \"1\", and \"2\". Also note that the levels of factors can be ordered, e.g. \"clothing size\" can be a factor, and you can order the levels \"small\", \"medium\", and \"large\", which is important when it comes to plotting the data.**\n\nFor our two SVMs, we will need to create the appropriate `x` and `y` variables based on `tree_data`.\n\nThe first SVM will be based on the leaf features `leaf_width` and `leaf_length`. We will need to create a new variable that contains only these two features, then convert it from a `data.frame` to a `matrix`, so it can be the input to the `x` argument in the `svm` function. We also need to convert the labels `tree_type` into a factor for the input to the `y` variable  of the `svm` function, as it is currently stored as an integer (see the results from `str` above, where `int` stands for `integer`).\n\nThe code below creates the appropriate `x` and `y` variables for the leaf features in `tree_data`. \n\n### In the cell below replace:\n#### 1. `<leafFeature1>` with `leaf_width`\n#### 2. `<leafFeature2>` with `leaf_length`\n#### then __run the code__."
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create new x variable input to `svm`\nx_leaf_data <- tree_data %>% \n\n###\n# REPLACE <leafFeature1> WITH leaf_width AND <leafFeature2> WITH leaf_length\n###\nselect(<leafFeature1>, <leafFeature2>) %>%\n###\nas.matrix()\n\n# Check x variable input to `svm`\nclass(x_leaf_data)\nhead(x_leaf_data)\n\n# Change `tree_data$tree_type` to a factor\ntree_data <- tree_data  %>% \nmutate(tree_type = as.factor(tree_type))\n\n# Check y variable input to `svm`\nclass(tree_data$tree_type)\nhead(tree_data$tree_type)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now we can run the function `svm` based on the leaf features stored in the new variable `x_leaf_data`, and the label saved in the variable `tree_data$tree_type`.\n\n### In the cell below replace:\n#### 1. `<features>` with `x_leaf_data`\n#### 2. `<labels>` with `tree_data$tree_type`\n#### then __run the code__."
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "###\n# REPLACE <features> WITH x_leaf_data AND <labels> WITH tree_data$tree_type\n###\nsvm_leaf_data <- svm(x = <features>, y = <labels>, type = \"C-classification\", kernel = \"radial\")\n###\nprint(\"The SVM model named svm_leaf_data is ready.\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To help us view the hyperplanes of the SVM based on the leaf data, we will create a fine grid of data points within the feature space to represent different combinations of leaf width and leaf length, and colour the new data points based on the predictions of `svm_leaf_data`. You do not need to edit this code block.\n\n**Run the code below**"
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Run this box to create the grid of datapoints\n\n# Create a fine grid of the feature space\nleaf_width <- seq(from = min(tree_data$leaf_width), to = max(tree_data$leaf_width), length = 100)\nleaf_length <- seq(from = min(tree_data$leaf_length), to = max(tree_data$leaf_length), length = 100)\n\nfine_grid_leaf <- as.data.frame(expand.grid(leaf_width, leaf_length))\nfine_grid_leaf <- fine_grid_leaf %>%\n                  dplyr::rename(leaf_width = \"Var1\", leaf_length = \"Var2\")\n\n# Check output\nhead(fine_grid_leaf)\n\n# For every new point in `fine_grid_leaf`, predict its tree type based on the SVM `svm_leaf_data`\nfine_grid_leaf$tree_pred <- predict(svm_leaf_data, newdata = fine_grid_leaf, type = \"decision\")\n\n# Check output\nhead(fine_grid_leaf)\ntable(fine_grid_leaf$tree_pred)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now we can create a scatter plot that contains the new fine grid of points we created above, and also the original tree data to see which group the different trees fall into based on the SVM `svm_leaf_data`. You do not need to edit this code block.\n\n**Run the code below**"
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Run this box to generate the scatter plot\n\n# Create scatter plot  with original leaf features layered over the fine grid of data points\nggplot() +\ngeom_point(data = fine_grid_leaf, aes(x = leaf_width, y = leaf_length, colour = tree_pred), alpha = 0.25) +\nstat_contour(data = fine_grid_leaf, aes(x = leaf_width, y = leaf_length, z = as.integer(tree_pred)),\n             lineend = \"round\", linejoin = \"round\", linemitre = 1, size = 0.25, colour = \"black\") +\ngeom_point(data = tree_data, aes(x = leaf_width, y = leaf_length, colour = tree_type, shape = tree_type)) +\nggtitle(\"SVM decision boundaries for leaf length vs. leaf width\") +\nlabs(x = \"Leaf width\", y = \"Leaf length\", colour = \"Actual tree type\", shape = \"Actual tree type\") +\ntheme(plot.title = element_text(hjust = 0.5))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The graph shows three faintly coloured zones based on the SVM's predictions for the fine grid of data points (based on leaf features), and the hyperplanes for the different tree types represented by thick black lines. \n\nWe can use these coloured zones and hyperplanes to observe which tree type the SVM has chosen to place our original data points into. Note that in the graph above, our original data points are represented by both colour and shape. Also remember, that the tree type of the fine grid of data points is based on the SVM model where we used leaf features as input to the SVM.\n\nSo, using the graph above, we observe two different classification scenarios:\n\n1. Our original data points are classified correctly by the SVM, as the data point falls into the zone of the same colour, e.g. a green triangle data point (an actual type 1 tree) falls into the green zone (the SVM predicted the tree as type 1).\n\n2. Our original data points are misclassified by the SVM, as the data point falls into the zone of a different colour, e.g. a red circle data point (an actual type 0 tree) falls into the green zone (the SVM predicted the tree as type 1). \n\nFor the most part, our SVM can calculate tree type based on leaf features reasonably well, but let's determine the mis-classification rate. To do this, we will need to run the `predict` function again, but this time using our original data points as input. Note that this method is somewhat circular, since we used this same data to train the SVM, but we will run this just to give us an idea how well our SVM fits our data. \n\n> **If we truly want to test the performance of our SVM, we need a *training set* with which to train the SVM, and an independent *test/validation set* with which to test the SVM.**\n\n**Run the code below, to run the `predict` function. You do not need to edit this code block.**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Run this box to run the predict function\n\npred_leaf_data <- tree_data %>% \nselect(leaf_width, leaf_length)\n\n# Predict the tree type of our original data based on the SVM `svm_leaf_data`\npred_leaf_data$tree_pred <- predict(svm_leaf_data, newdata = pred_leaf_data, type = \"decision\")\n\n# Check output\nhead(pred_leaf_data)\n\n# Add tree_data$tree_type to pred_leaf_data\npred_leaf_data <- inner_join(pred_leaf_data, tree_data, by = c(\"leaf_width\", \"leaf_length\")) %>%\nselect(-trunk_girth, -trunk_height)\n\n# Check output\nhead(pred_leaf_data)\n\n# Create a table of predictions to show mis-classification rate\ntable(pred_leaf_data$tree_pred, pred_leaf_data$tree_type)\n\n# Mis-classification rate: proportion of misclassifiedb observations\nmean(pred_leaf_data$tree_pred != pred_leaf_data$tree_type)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Our mis-classification rate is 6.5% which can actually preferable to a mis-classification rate of 0%, as the latter might indicate that the model has overfit the training data.\n\n# Step 3\n\nNow let's create our second SVM based on the trunk features. Remember, for the `e1071::svm` function, we need to create a new variable for input to the `x` argument, but we can use the same variable as before as input to `y`, `tree_data$tree_type`.\n\n### In the cell below replace:\n#### 1. `<trunkFeature1>` with `trunk_girth`\n#### 2. `<trunkFeature2>` with `trunk_height`\n#### then __run the code__."
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create new x variable input to `svm` based on trunk features\nx_trunk_data <- tree_data %>% \n\n###\n# REPLACE <trunkFeature1> WITH trunk_girth and <trunkFeature2> WITH trunk_height\n###\nselect(<trunkFeature1>, <trunkFeature2>) %>%\n###\nas.matrix()\n\n# Check output\nhead(x_trunk_data)\n\n# Fit SVM\nsvm_trunk_data <- svm(x = x_trunk_data, y = tree_data$tree_type, type = \"C-classification\", kernel = \"radial\")\n\n# Create a fine grid of the feature space\ntrunk_girth <- seq(from = min(tree_data$trunk_girth), to = max(tree_data$trunk_girth), length = 100)\ntrunk_height <- seq(from = min(tree_data$trunk_height), to = max(tree_data$trunk_height), length = 100)\n\nfine_grid_trunk <- as.data.frame(expand.grid(leaf_width, leaf_length))\nfine_grid_trunk <- fine_grid_trunk %>% \n                   dplyr::rename(trunk_girth = \"Var1\", trunk_height = \"Var2\")\n\n# Check output\nhead(fine_grid_trunk)\n\n# Predict which tree type the new points fall into\nfine_grid_trunk$tree_pred <- predict(svm_trunk_data, newdata = fine_grid_trunk, type = \"decision\")\n\n# Check output\nhead(fine_grid_trunk)\ntable(fine_grid_trunk$tree_pred)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now let's create a scatter plot using `ggplot2`. We will plot the fine grid as well as the original tree points.\n\n### In the cell below replace:\n#### 1. `<data1>` with `fine_grid_trunk`\n#### 2. `<data2>` with `fine_grid_trunk`\n#### 3. `<data3>` with `tree_data`\n#### then __run the code__."
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create scatter plot with original trunk features layered over the fine grid of data points\nggplot() +\n\n# First plot the fine grid of data points;\n###\n# REPLACE <data1> WITH fine_grid_trunk\n###\ngeom_point(data = <data1>, aes(x = trunk_girth, y = trunk_height, colour = tree_pred), alpha = 0.25) +\n\n# Add contour lines based on fine grid of data points; \n###\n# REPLACE <data2> WITH fine_grid_trunk\n###\nstat_contour(data = <data2>, aes(x = trunk_girth, y = trunk_height, z = as.integer(tree_pred)),\n             lineend = \"round\", linejoin = \"round\", linemitre = 1, size = 0.25, colour = \"black\") +\n###\n\n# Now plot the original data points to see where they lie in relation to the fine grid of data points;\n\n###\n# REPLACE <data3> WITH tree_data\n###\ngeom_point(data = <data3>, aes(x = trunk_girth, y = trunk_height, colour = tree_type, shape = tree_type)) +\n###\nggtitle(\"SVM decision boundaries for trunk girth vs. trunk height\") +\nlabs(x = \"Trunk girth\", y = \"Trunk height\", colour = \"Tree type\", shape = \"Tree type\") +\ntheme(plot.title = element_text(hjust = 0.5))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Excellent! Again we can observe three faintly coloured zones based on the SVM's predictions of tree type for the fine grid of data points (based on trunk features), and the hyperplanes for the different tree types represented by thick black lines. We use these coloured zones and hyperplanes to observe which tree type the SVM has chosen to place our original data points into. Again, we observe two different classification scenarios: either our original data points are classified correctly by the SVM, or 2) our original data points are misclassified by the SVM.\n\n**Now let's run the `predict` function as we did earlier to determine the mis-classification rate of our SVM model based on trunk features.**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Run this box to determing the mis-classification rate\n\npred_trunk_data <- tree_data %>% \nselect(trunk_girth, trunk_height)\n\n# Predict the tree type of our original data based on the SVM `svm_trunk_data`\npred_trunk_data$tree_pred <- predict(svm_trunk_data, newdata = pred_trunk_data, type = \"decision\")\n\n# Check output\nhead(pred_trunk_data)\n\n# Add tree_data$tree_type to pred_trunk_data\npred_trunk_data <- inner_join(pred_trunk_data, tree_data, by = c(\"trunk_girth\", \"trunk_height\")) %>%\nselect(-leaf_length, -leaf_width)\n\n# Check output\nhead(pred_trunk_data)\n\n# Create a table of predictions to show mis-classification rate\ntable(pred_trunk_data$tree_pred, pred_trunk_data$tree_type)\n\n# Mis-classification rate: proportion of misclassifiedb observations\nmean(pred_trunk_data$tree_pred != pred_trunk_data$tree_type)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Here our mis-classification rate of the training data using the `svm_trunk_data` model is 4.5%, which is lower than the mis-classification rate of the `svm_leaf_data` model.\n\n\nConclusion\n-------\n\nThat's it! You've made two simple SVMs that can predict the type of tree based on the leaf measurements and trunk measurements!\n\nYou can go back to the course now and click __'Next Step'__ to move onto how we can test AI models."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "r",
      "display_name": "R",
      "language": "R"
    },
    "language_info": {
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": "3.4.1",
      "file_extension": ".r",
      "codemirror_mode": "r"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}